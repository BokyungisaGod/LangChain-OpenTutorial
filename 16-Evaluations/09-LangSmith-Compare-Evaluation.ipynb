{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# LangSmith-Compare-Evaluation\n",
        "\n",
        "- Author: [BokyungisaGod](https://github.com/BokyungisaGod)\n",
        "- Design: \n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "You can easily compare experimental results using the **Compare** feature provided by `LangSmith`. This tutorial demonstrates how to evaluate the performance of models like `GPT-4o-mini` and `Ollama` in a RAG system, enabling you to analyze their ability to generate accurate, context-based answers.\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Define-a-function-for-rag-performance-testing](#define-a-function-for-rag-performance-testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
        "\n",
        "\n",
        "**[Note]**\n",
        "\n",
        "The langchain-opentutorial is a package of easy-to-use environment setup guidance, useful functions and utilities for tutorials.\n",
        "Check out the  [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain_openai\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can set API keys in a `.env` file or set them manually.\n",
        "\n",
        "[Note] If you’re not using the `.env` file, no worries! Just enter the keys directly in the cell below, and you’re good to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "327c2c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
        "if not load_dotenv():\n",
        "    set_env(\n",
        "        {\n",
        "            \"OPENAI_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "            \"LANGCHAIN_PROJECT\": \"LangSmith-Compare-Evaluation\",  # set the project name same as the title\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa00c3f4",
      "metadata": {},
      "source": [
        "## Define a function for RAG performance testing\n",
        "Create a RAG system to use for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "16c769f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from myrag import PDFRAG\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Create a function to answer the question\n",
        "def ask_question_with_llm(llm):\n",
        "    # Create PDFRAG object\n",
        "    rag = PDFRAG(\n",
        "        \"data/Newwhitepaper_Agents2.pdf\",\n",
        "        llm,\n",
        "    )\n",
        "\n",
        "    # Create retriever\n",
        "    retriever = rag.create_retriever()\n",
        "\n",
        "    # Create chain\n",
        "    rag_chain = rag.create_chain(retriever)\n",
        "\n",
        "    def _ask_question(inputs: dict):\n",
        "        context = retriever.invoke(inputs[\"question\"])\n",
        "        context = \"\\n\".join([doc.page_content for doc in context])\n",
        "        return {\n",
        "            \"question\": inputs[\"question\"],\n",
        "            \"context\": context,\n",
        "            \"answer\": rag_chain.invoke(inputs[\"question\"]),\n",
        "        }\n",
        "\n",
        "    return _ask_question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "895262cf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-01-14T17:11:32.308467Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9318028375, 'load_duration': 1418234667, 'prompt_eval_count': 26, 'prompt_eval_duration': 6607000000, 'eval_count': 10, 'eval_duration': 1283000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-bee3f443-5d6a-4237-81c5-143f2267c99e-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Load the Ollama model.\n",
        "ollama = ChatOllama(model=\"llama3.2\")\n",
        "\n",
        "# Call the Ollama model\n",
        "ollama.invoke(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "439d68fb",
      "metadata": {},
      "source": [
        "Create a function that generates answers to questions using the `GPT-4o-mini` model and the `llama` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "31a93c74",
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
        "ollama_chain = ask_question_with_llm(ChatOllama(model=\"llama3.2\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca68964d",
      "metadata": {},
      "source": [
        "Conduct answer evaluation using the `GPT-4o-mini` model and the `llama` model.\n",
        "\n",
        "Perform the evaluation for each of the two chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "30abf2b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'MODEL_COMPARE_EVAL-bd816470' at:\n",
            "https://smith.langchain.com/o/9089d1d3-e786-4000-8468-66153f05444b/datasets/9b4ca107-33fe-4c71-bb7f-488272d895a3/compare?selectedSessions=fbb203cd-fe4a-4982-b7be-95c657794165\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9d02a7219d14a5781464002b35bb046",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'MODEL_COMPARE_EVAL-660d90ec' at:\n",
            "https://smith.langchain.com/o/9089d1d3-e786-4000-8468-66153f05444b/datasets/9b4ca107-33fe-4c71-bb7f-488272d895a3/compare?selectedSessions=09fbec65-fe5c-4559-84d3-ad6ce705255a\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3819e918a2d4a95b137cd87c77254b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
        "\n",
        "# Create a QA evaluator\n",
        "cot_qa_evalulator = LangChainStringEvaluator(\n",
        "    \"cot_qa\",\n",
        "    config={\"llm\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)},\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"answer\"],\n",
        "        \"reference\": run.outputs[\"context\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "dataset_name = \"RAG_EVAL_DATASET\"\n",
        "\n",
        "# Execute the evaluation\n",
        "experiment_results1 = evaluate(\n",
        "    gpt_chain,\n",
        "    data=dataset_name,\n",
        "    evaluators=[cot_qa_evalulator],\n",
        "    experiment_prefix=\"MODEL_COMPARE_EVAL\",\n",
        "    # Specify experiment metadata\n",
        "    metadata={\n",
        "        \"variant\": \"GPT-4o-mini evaluation (cot_qa)\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Execute the evaluation\n",
        "experiment_results2 = evaluate(\n",
        "    ollama_chain,\n",
        "    data=dataset_name,\n",
        "    evaluators=[cot_qa_evalulator],\n",
        "    experiment_prefix=\"MODEL_COMPARE_EVAL\",\n",
        "    # Specify experiment metadata\n",
        "    metadata={\n",
        "        \"variant\": \"Ollama(llama3.2) evaluation (cot_qa)\",\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d519c12",
      "metadata": {},
      "source": [
        "Use comparison view to check the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d8ce072",
      "metadata": {},
      "source": [
        "### How to view comparison\n",
        "\n",
        "1. Select the experiments you want to compare in the Experiment tab of the Dataset.\n",
        "2. Click the \"Compare\" button at the bottom.\n",
        "3. The comparison view will be displayed.\n",
        "\n",
        "![](./assets/09-LangSmith-Compare-Evaluation-01.png)\n",
        "\n",
        "![](./assets/09-LangSmith-Compare-Evaluation-02.png)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-kr-lwwSZlnu-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
